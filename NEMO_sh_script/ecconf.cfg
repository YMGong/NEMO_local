#!/bin/sh

function configure()
{
    # This function should configure all settings/modules needed to
    # later prepare the EC-Earth run directory and set variables used
    # in the run script

    # Configure paths for building/running EC-Earth
    ecearth_src_dir=${ECEARTH3_BASE_DIR}/sources
    run_dir=${WRKDIR}/EC-Earth/${exp_name}
    ini_data_dir=/proj/atm/EC-Earth/input-trunk-r4062

    if [ ! -d $ecearth_src_dir ]; then
	echo "ECEARTH_SRC_DIR in config_run.xml is not a directory!"
	exit 1
    fi
    if [ ! -d $ini_data_dir ]; then
	echo "INI_DATA_DIR in config_run.xml is not a directory!"
	exit 1
    fi

    # File for standard output.
    # NOTE: This will be modified for restart jobs!
    stdout_file=${start_dir}/out/$(basename ${SLURM_JOB_NAME}).out

    # Resubmit this job for automatic restarts? [true/false]
    # Also, add options for the resubmit command here.
    resubmit_job=false
    resubmit_opt=""

    # Configure GRIBEX paths
    export LOCAL_DEFINITION_TEMPLATES=/appl/climate/gribex/nonCrayOriginalF2C/GNU/5.3/lib/gribtemplates

    # Configure GRIB API paths
    export GRIB_DEFINITION_PATH=/appl/climate/grib_api/1.23.1/CRAY/8.5/share/grib_api/definitions
    export GRIB_SAMPLES_PATH=/appl/climate/grib_api/1.23.1/CRAY/8.5/share/grib_api/ifs_samples/grib1
    export GRIB_BIN_PATH=/appl/climate/grib_api/1.23.1/CRAY/8.5/bin

    # Configure number of processors per node
    proc_per_node=24

    # Configure and load modules
    pre_load_modules_cmd=""
    module_list=""

    if [ -n "${module_list}" ]
    then
        set +u
        if [ -n "${pre_load_modules_cmd}" ]
        then
            ${pre_load_modules_cmd}
        fi
        for m in "${module_list}"
        do
            eval $(/usr/libexec/cmod sh add $m)
        done
        set -u
    fi

    # Add directories to the shared library search path
    if [ -n "" ]
    then
        export LD_LIBRARY_PATH=${LD_LIBRARY_PATH:+${LD_LIBRARY_PATH}:}""
    fi

    ulimit -s unlimited
}

function launch()
{
    # This function should launch the execution of a coupled experiment and handle
    # any configuration that for some reason couldnt go into the configuration
    # function

    (( nnodes = ( oas_numproc -1 ) / proc_per_node + \
     	( ifs_numproc -1 ) / proc_per_node + \
	( nem_numproc -1 ) / proc_per_node + 3 )) 

    echo "oas_numproc: $oas_numproc"
    echo "ifs_numproc: $ifs_numproc"
    echo "nem_numproc: $nem_numproc"
    echo "proc_per_node: $proc_per_node"
    echo "nnodes: $nnodes"

    if [ $SLURM_JOB_NUM_NODES -ne $nnodes ]; then
	error "SLURM_JOB_NUM_NODES=$SLURM_JOB_NUM_NODES -ne nnodes=$nnodes"
    fi

run_script=run.sh
aprun -n ${oas_numproc} ./$(basename ${oas_exe_file}) : \
      -n ${ifs_numproc} ./$(basename ${ifs_exe_file}) -v ecmwf -e $exp_name : \
      -n ${nem_numproc} ./$(basename ${nem_exe_file})

}

function launch_atm()
{
    # This function should launch the execution of an atmospheric-only experiment and
    # handle any configuration that for some reason couldnt go into the configuration
    # function

    (( nnodes = ( ifs_numproc -1 ) / proc_per_node + 1 ))

    echo "ifs_numproc: $ifs_numproc"
    echo "proc_per_node: $proc_per_node"
    echo "nnodes: $nnodes"

    if [ $SLURM_JOB_NUM_NODES -ne $nnodes ]; then
	error "SLURM_JOB_NUM_NODES=$SLURM_JOB_NUM_NODES -ne nnodes=$nnodes"
    fi

run_script=run-ifs.sh
aprun -n ${ifs_numproc} ./$(basename ${ifs_exe_file}) -v ecmwf -e $exp_name

    $cmd
}

function launch_oce()
{
    # This function should launch the execution of an ocean-only experiment and
    # handle any configuration that for some reason couldnt go into the configuration
    # function

    (( nnodes = ( nem_numproc -1 ) / proc_per_node + 1 ))

    echo "nem_numproc: $nem_numproc"
    echo "proc_per_node: $proc_per_node"
    echo "nnodes: $nnodes"

    if [ $SLURM_JOB_NUM_NODES -ne $nnodes ]; then
	error "SLURM_JOB_NUM_NODES=$SLURM_JOB_NUM_NODES -ne nnodes=$nnodes"
    fi

run_script=run-oce.sh
aprun -n ${nem_numproc} ./$(basename ${nem_exe_file})

}

function finalise()
{
    # This function should execute of any post run functionality, e.g.
    # platform dependent cleaning or a resubmit

    if ${resubmit_job} && [ $(date -d "${leg_end_date}" +%s) -lt $(date -d "${run_end_date}" +%s) ]
    then
        info "Resubmitting job for leg $((leg_number+1))"
        # Need to go to start_dir to find the run script
        cd ${start_dir}
        # Submit command
        # Note: This does not work of you specify a job name with sbatch -J jobname!
        sbatch -N ${SLURM_JOB_NUM_NODES}                                                 \
               -o ${run_dir}/$(basename ${stdout_file}).$(printf %03d $((leg_number+1))) \
               -e ${run_dir}/$(basename ${stdout_file}).$(printf %03d $((leg_number+1))) \
               -d ${SLURM_JOB_ID}                                                        \
               ${resubmit_opt}                                                           \
               ./${SLURM_JOB_NAME}
    fi
}
